清理：sh clean.sh 20141025 20141113 >> clean_cmd.txt
git clone https://github.com/elex-bigdata/feudf.git

https://github.com/elex-bigdata/FeatureExtraction.git
https://github.com/elex-bigdata/FEComm.git

add jar /home/hadoop/wuzhongju/ssp/feUDF-1.0.jar;




CREATE TEMPORARY FUNCTION area AS 'com.elex.ssp.udf.IPDim';
CREATE TEMPORARY FUNCTION timefunc as 'com.elex.ssp.udf.TimeDim';
CREATE TEMPORARY FUNCTION qn AS 'com.elex.ssp.udf.Query';
CREATE TEMPORARY FUNCTION wc AS 'com.elex.ssp.udf.WordCount';
CREATE TEMPORARY FUNCTION ql AS 'com.elex.ssp.udf.QueryLength';
CREATE TEMPORARY FUNCTION qs AS 'com.elex.ssp.udf.QuerySplit';
CREATE TEMPORARY FUNCTION concatcolon AS 'com.elex.ssp.udf.GroupConcatColon';
CREATE TEMPORARY FUNCTION concatspace AS 'com.elex.ssp.udf.GroupConcatSpace';
CREATE TEMPORARY FUNCTION sed as 'com.elex.ssp.udf.KeyWord';

add jar /home/hadoop/git_project_home/feudf/feUDF/target/feUDF-1.0.jar;
CREATE TEMPORARY FUNCTION cadid AS 'com.elex.ssp.udf.ChooseAdid';
CREATE TEMPORARY FUNCTION cdt AS 'com.elex.ssp.udf.ChooseDt';

select qn(keyword) as col1,ql(keyword) as col2,wc(keyword) as col2 from search where day ='20141021' and nation='ph' limit 10;


CREATE TEMPORARY FUNCTION timefunc as 'com.elex.ssp.udf.TimeDim';
select timefunc("2014-10-09 13:20:01","us") as col1 from dual;
DROP  TEMPORARY  FUNCTION timefunc;

CREATE TEMPORARY FUNCTION sed as 'com.elex.ssp.udf.KeyWord';
select sed(keyword) as col1 from search where day ='20141021' and nation='ph' limit 10;

======================准备输入数据==============================
INSERT OVERWRITE LOCAL DIRECTORY '/home/hadoop/test' row format delimited fields terminated by ',' stored as textfile
array_contains(array('in','us','pk','ph','gb','au','za','lk','ca','sg','sg','nz','ie','ng','gh','cm'),s.nation)

#齐全备料表
create table log_merge(reqid string,uid string,pid string,ip string,nation string,ua string,os string,width string,height string,pv int,adid string,impr int,time string,click int,query string,sv int) partitioned by(day string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;

(SELECT reqid,MAX(uid) as uid,MAX(pid) as pid,MAX(ip) as ip,MAX(nation) as nation,MAX(ua) as ua,MAX(os) as os,MAX(width) as width,MAX(height) as height,1 AS pv FROM nav_visit WHERE DAY = '20141022' GROUP BY reqid )a

(SELECT reqid,adid,MAX(time)as time,COUNT(uid) AS impr FROM ad_impression WHERE DAY='20141022' GROUP BY reqid,adid)b

(SELECT reqid,COUNT(1) AS click FROM ad_click WHERE DAY ='20141022' GROUP BY reqid)c

(SELECT reqid,CONCAT_WS(' ',collect_set(qn(keyword))) AS q,COUNT(uid) AS sv FROM search WHERE DAY='20141022' GROUP BY reqid)d

#产生齐全备料表, 假设每个页面至少有一个广告，则从广告展现表为基表进行关联数据是完整的，如果页面没有广告但有搜索，这种统计方式会漏掉部分页面展示pv和搜索日志。
#另外，由于目前广告点击无法计量adid，一旦一个页面有多个广告，目前的统计方式会导致广告点击量重复记录，需要改变join表b到表c的join方式，增加adid的等值关联。
insert overwrite table log_merge partition(day='20141022')
 SELECT b.reqid,a.uid,a.pid,a.ip,a.nation,a.ua,a.os,a.width,a.height,a.pv,b.adid,b.impr,b.time,c.click,d.q,d.sv FROM (SELECT reqid,adid,MAX(time)as time,COUNT(uid) AS impr FROM ad_impression WHERE DAY='20141022' GROUP BY reqid,adid)b LEFT OUTER JOIN (SELECT reqid,MAX(uid) as uid,MAX(pid) as pid,MAX(ip) as ip,MAX(nation) as nation,MAX(ua) as ua,MAX(os) as os,MAX(width) as width,MAX(height) as height,COUNT(1) AS pv FROM nav_visit WHERE DAY = '20141022' GROUP BY reqid )a ON a.reqid=b.reqid LEFT OUTER JOIN (SELECT reqid,COUNT(1) AS click FROM ad_click WHERE DAY ='20141022' GROUP BY reqid)c ON c.reqid=b.reqid LEFT OUTER JOIN (SELECT reqid,CONCAT_WS(':',collect_set(qn(keyword))) AS q,COUNT(uid) AS sv FROM search WHERE DAY='20141022' GROUP BY reqid)d ON d.reqid=b.reqid

#英语国家搜索日志表
create table query_en(reqid string,uid string,pid string,query string,nation string,adid string,pv int,impr int,sv int,click int) partitioned by(day string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;

#产生英语国家搜索日志
insert overwrite table query_en partition(day='20141022')
 select reqid,uid,pid,tab.col1,nation,adid,pv,impr,sv,click from log_merge lateral view qs(query,':') tab as col1 where day ='20141022' and array_contains(array('in','us','pk','ph','gb','au','za','lk','ca','sg','sg','nz','ie','ng','gh','cm'),nation) and query is not null;

#关键词的tfidf表
create table tfidf(uid string,word string,wc int,tf double,idf double,tfidf double)partitioned by(day string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;

================================输入数据准备完成===================================

======================中间结果表 start===============================
#中间结果表，按特征类型和日期分区。特征类型有：query,query_length,query_word_count,keyword,time,area,browser,user,project
#fv(特征值),ft(特征类型),nv(nav_visit_count),sv(search_count),impr(广告展示量),click(广告点击量)
create table feature(fv string,nation string,adid string,pv int,sv int,impr int,click int) partitioned by(day string,ft string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;
#profile表中特征类型为user时,fv值为null,标识该日该用户的pv、sv、impr、click合计
create table profile(uid string,fv string,nation string,pv int,sv int,impr int,click int) partitioned by(day string,ft string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;
=====================中间结果表   end=================================

词频（TF）=某关键词出现次数/文章中关键词总数
逆文档频率（IDF）=log（语料库文档总数/（包含该词的文档数+1））

 
==============================每日处理========================
#时间维度
add jar /home/hadoop/wuzhongju/ssp/feUDF-1.0.jar;
CREATE TEMPORARY FUNCTION tf as 'com.elex.ssp.udf.TimeDim';
insert overwrite table feature partition(day='20141022',ft='time') 
 select tab.col1,nation,adid,sum(pv),sum(sv),sum(impr),sum(click) from log_merge lateral view tf(time,nation) tab as col1 
 where day ='20141022' and time is not null and nation is not null  group by tab.col1,nation,adid
 
#地区维度
CREATE TEMPORARY FUNCTION area AS 'com.elex.ssp.udf.IPDim'
insert overwrite table feature partition(day='20141022',ft='area') 
 select area(ip) as a,nation,adid,sum(pv),sum(sv),sum(impr),sum(click) from log_merge  
 where day ='20141022' and ip is not null  group by a,nation,adid 
==============================每日处理========================

==============================feature合并表========================
create table feature_merge(ft string,fv string,nation string,adid string,pv int,sv int,impr int,click int,ctr1 double,ctr2 double,ad_fill double) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;
==============================feature合并表========================

====================profile合并表======================
create table profile_merge(uid string,ft string,fv string,nation string,pv int,sv int,impr int,click int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;
====================profile合并表=======================

=================结果按条件导出（示例语句）==========
SELECT 
  gid,
  lang,
  rate 
FROM
  hb_count 
WHERE 
  CASE
    lang 
    WHEN 'pt' 
    THEN rate > 0.417 
    ELSE rate > 0 
  END 
  
  
  SELECT 
  gid,
  lang,
  CASE
    WHEN rate>0.4 AND rate <3 THEN 'level1'
    WHEN rate>=3 AND rate <7 THEN 'level2'
    WHEN rate>=7 AND rate <10 THEN 'level3'
    ELSE 'noLevel'
  END  AS LEVEL
FROM
  hb_count 
  
  
=====================分析========================
INSERT OVERWRITE LOCAL DIRECTORY '/home/hadoop/ssp_feature_ana' ROW FORMAT delimited FIELDS TERMINATED BY ',' stored AS textfile
select a.ft,a.nation,a.adid,a.attr,sum(a.wc),sum(a.pv),sum(a.impr),sum(a.sv),sum(a.click) from(
SELECT 
  ft,nation,adid,
  CASE
  ft
    WHEN 'user' THEN 'all'
    WHEN 'time' THEN fv
    WHEN 'area' THEN 'all'
    WHEN 'browser' THEN fv
    WHEN 'project' THEN fv
    WHEN 'query' THEN 'all'
    WHEN 'query_length' THEN fv
    WHEN 'query_word_count' THEN fv
    WHEN 'keyword' THEN 'all'
    ELSE 'all'
  END  as attr,1 as wc,pv,impr,sv,click FROM odin.feature_merge)a group by a.ft,a.nation,a.adid,a.attr
  
==========================导出===========================================

select ft,fv,nation,adid,pv,impr,click,ctr1,ctr2,ad_fill from feature_merge where fv is not null  and case ft when 'user' then pv>20 when 'keyword' then pv >100 when 'query' then length(fv)>3 end limit 100;
  
nations='in','us','pk','ph','gb','au','za','lk','ca','sg','sg','nz','ie','ng','gh','cm'

ALTER TABLE ad_impression ADD COLUMNS (dt STRING);
ALTER TABLE log_merge ADD COLUMNS (dt STRING);
ALTER TABLE log_merge2 ADD COLUMNS (dt STRING);
ALTER TABLE query_en ADD COLUMNS (dt STRING);
ALTER TABLE query_en2 ADD COLUMNS (dt STRING);
ALTER TABLE feature ADD COLUMNS (dt STRING);
ALTER TABLE profile ADD COLUMNS (dt STRING);

====================gdp=========================
create table gdp_daysearch(uid string,query string) partitioned by(day string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;

create table gdp_search_merge(uid string,query string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;

add jar /home/hadoop/wuzhongju/ssp/feUDF-1.0.jar;
CREATE TEMPORARY FUNCTION concatspace AS 'com.elex.ssp.udf.GroupConcatSpace';
CREATE TEMPORARY FUNCTION qn AS 'com.elex.ssp.udf.Query';

set mapred.reduce.tasks = 20;

INSERT OVERWRITE TABLE gdp_search 
SELECT 
  a.uid,
  concatspace(a.q) 
FROM
  (SELECT 
    uid,
    CASE
      WHEN url RLIKE '.*q=(.*?)(\\&\\.*)' 
      THEN qn(regexp_extract(url, '.*q=(.*?)(\\&\\.*)', 1))
      ELSE qn(regexp_extract(url, '.*q=(.*)', 1))
    END AS q 
  FROM
    odin.gdp 
  WHERE url LIKE '%google.com%' 
    AND url LIKE '%q=%' 
    AND DAY > '20141026' 
    AND nation = 'br' 
    AND LOWER(SUBSTR(lang, 0, 2)) = 'en') a  
  GROUP BY a.uid;
  
#hive更改表名  
alter table gdp_search rename to gdp_search_merge;


select count(1) from (select a.uid from(select distinct uid from query_en2 where day>'20141101')a join (select distinct uid from gdp_daysearch)b on a.uid=b.uid)t
1192078:214670:19160



=================gdp_odp=========================
java -cp .:odp.jar:dmoz.txt com.elex.ssp.odp.DmozUtils /data1/yac_url/content.rdf.u8 /data1/yac_url/rdf.csv >> error.txt

cat rdf.csv | awk '{print $1}' >> a.txt

create table odp_init(host string,category string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;

LOAD DATA LOCAL INPATH '/data1/yac_url/a.txt' overwrite into table odp_init;

select category,count(1) as c from odp_init group by category order by c desc;

create table odp(host string,category string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;

INSERT overwrite TABLE odp 
SELECT 
  a.host,
  CASE
    WHEN size (split (a.c, ':')) > 3 
    THEN 
    CASE
      WHEN a.host LIKE '%.edu%' 
      THEN 'Education' 
      ELSE 'Composite' 
    END 
    ELSE a.c
  END 
FROM
  (SELECT 
    HOST,
    CONCAT_WS(':', collect_set (category)) AS c 
  FROM
    odp_init 
  GROUP BY HOST) a
  
=====odp分析=============
select count(distinct category) from odp where size(split(category,':'))>1; 3053

select count(distinct category) from odp; 3199


drop table odp_init;

create table gdp_odp(uid string,nation string,host string,category string,visit int) partitioned by(day string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;

====使用. 作为split分隔符时候要加上\\,否则hive会将.解释为正则中的特殊字符===========
INSERT OVERWRITE TABLE gdp_odp PARTITION (DAY = '20141118') 
SELECT 
  t.uid,
  t.nation,
  t.domain,
  o.category,
  t.visit 
FROM
  odp o 
  RIGHT OUTER JOIN 
    (SELECT 
      a.uid,
      a.nation,
      a.domain,
      COUNT(1) AS visit 
    FROM
      (SELECT 
        regexp_replace(uid,',','') as uid,
        regexp_replace(nation,',','') as nation,
        parse_url (regexp_replace(url,',',''), 'HOST') AS domain 
      FROM
        odin.gdp 
      WHERE DAY = '20141118') a 
    WHERE a.domain IS NOT NULL 
      AND NOT(a.domain RLIKE '\\d+\\.\\d+\\.\\d+\\.\\d+') 
      AND size (split (a.domain, '\\.')) >= 2 
    GROUP BY a.uid,a.nation,a.domain) t 
    ON t.domain = o.host  
    
====gdp_odp分析======
select day,case when category is not null then 'yes' else 'no' end,sum(visit) from gdp_odp group by case when category is not null then 'yes' else 'no' end,day;

create table gdp_utag(uid string,nation string,tag string,visit int)partitioned by(day string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;

add jar /home/hadoop/wuzhongju/ssp/feUDF-1.0.jar;
CREATE TEMPORARY FUNCTION qs AS 'com.elex.ssp.udf.QuerySplit';

INSERT overwrite TABLE gdp_utag PARTITION (DAY = '20141118') 
SELECT 
  uid,
  nation,
  tab.col1,
  SUM(visit) 
FROM
  gdp_odp lateral VIEW qs (category, ':') tab AS col1 
WHERE category IS NOT NULL 
  AND array_contains (array ('br'), nation) 
GROUP BY uid,
  nation,
  tab.col1     

====gdp_utag分析=====
select tag,count(distinct uid),sum(visit) from gdp_utag group by tag;

create table gdp_utag_merge(uid string,nation string,tag string,visit int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;

INSERT overwrite TABLE gdp_utag_merge 
 select uid,nation,tag,sum(visit) from gdp_utag group by uid,nation,tag

INSERT OVERWRITE DIRECTORY '/ssp/odp/tf'
 select m.uid,m.tag,m.visit,round(m.visit/t.vs,4) from gdp_utag_merge m join(select uid,sum(visit) as vs from gdp_utag_merge where array_contains (array ('br'), nation) group by uid)t on m.uid=t.uid where array_contains (array ('br'), m.nation)
 
INSERT OVERWRITE DIRECTORY '/ssp/odp/idf'
 select m.tag,round(log10(t.uc/m.tuc),4) from (select tag,count(distinct uid) as tuc from gdp_utag_merge group by tag)m join (select count(distinct uid) as uc from gdp_utag_merge) t on 1=1;  

create table user_tag(uid string,tag string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;

INSERT INTO TABLE feature_merge SELECT 'tag',t.tag,t.nation,t.adid,SUM(t.pv),SUM(t.sv),SUM(t.impr),SUM(t.click),ROUND(CASE WHEN SUM(t.click) IS NULL OR SUM(t.pv) IS NULL THEN 0 ELSE SUM(t.click)/SUM(t.pv) END,4),ROUND(CASE WHEN SUM(t.click) IS NULL OR SUM(t.pv) IS NULL THEN 0 ELSE SUM(t.click)/SUM(t.pv) END,4),ROUND(CASE WHEN SUM(t.impr) IS NULL OR SUM(t.pv) IS NULL THEN 0 ELSE SUM(t.impr)/SUM(t.pv) END,4) FROM(SELECT /*+ MAPJOIN(b) */ b.tag,a.* FROM user_tag b JOIN (SELECT fv,nation,adid,pv,sv,impr,click FROM feature_merge WHERE ft='user')a  ON a.fv=b.uid)t GROUP BY t.tag,t.nation,t.adid



